{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "1. max \n",
    "2. mean\n",
    "3. median\n",
    "4. percentile\n",
    "5. top k mean \n",
    "6. scale trimmed mean \n",
    "7. huber estimator\n",
    "8. generalized mean (2)\n",
    "9. log sum exp \n",
    "10. high quantile gaussian\n",
    "11. high quantile gpd \n",
    "12. gmm cluster\n",
    "13. empirical bayes\n",
    "14. EVT \n",
    "15. empirical bayes + EVT\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tahir/workspace2/miniconda3/envs/smoothquant/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats, optimize\n",
    "\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.683658  ,  0.65870637,  0.76537055,  1.0086393 ,  1.9101545 ,\n",
       "        0.5082128 , -0.56180954, -0.72310513,  0.7939475 ,  0.09072132,\n",
       "       -1.2164989 ,  0.20438556, -0.6047317 , -1.7558211 ,  0.45578036,\n",
       "       -2.2469485 ,  1.8344615 , -1.7856308 ,  1.1030827 , -0.34027243],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(20)\n",
    "x = x.numpy() \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2469485"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. max \n",
    "def scale_max(x):\n",
    "    return np.max(np.abs(x))\n",
    "\n",
    "y = scale_max(x) \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.962597"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. mean\n",
    "def scale_mean(x):\n",
    "    return np.mean(np.abs(x))\n",
    "\n",
    "y = scale_mean(x) \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74423784"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. median\n",
    "def scale_median(x):\n",
    "    return np.median(np.abs(x))\n",
    "\n",
    "y = scale_median(x) \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.240549394249917"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. percentile \n",
    "def scale_percentile(x, p=99.9):\n",
    "    return np.percentile(np.abs(x), p)\n",
    "\n",
    "y = scale_percentile(x) \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1402097"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def scale_rms(x):\n",
    "    return np.sqrt(np.mean(x**2))\n",
    "\n",
    "y = scale_rms(x) \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9066032"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. top k mean \n",
    "def scale_topk_mean(x, k=5):\n",
    "    x_sorted = np.sort(np.abs(x))\n",
    "    return np.mean(x_sorted[-k:])\n",
    " \n",
    "y = scale_topk_mean(x) \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9625969"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. scale trimmed mean\n",
    "def scale_trimmed_mean(x, trim_frac=0.02):\n",
    "    x_sorted = np.sort(np.abs(x))\n",
    "    n = len(x_sorted)\n",
    "    k = int(trim_frac * n)\n",
    "    trimmed = x_sorted[k:n-k]\n",
    "    return np.mean(trimmed)\n",
    "\n",
    "y = scale_trimmed_mean(x) \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7534882039763033"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. huber estimator\n",
    "def huber_estimator(x, delta=1.35, tol=1e-6, max_iter=50):\n",
    "    x = np.asarray(x)\n",
    "    mu = np.median(x)   # good robust initial guess\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        r = x - mu\n",
    "        w = np.where(np.abs(r) <= delta, 1, delta / np.abs(r))\n",
    "        mu_new = np.sum(w * x) / np.sum(w)\n",
    "        if abs(mu_new - mu) < tol:\n",
    "            break\n",
    "        mu = mu_new\n",
    "    \n",
    "    # Robust scale (like std but weighted)\n",
    "    r = x - mu\n",
    "    scale = np.sqrt(np.mean(np.minimum(r**2, (delta**2))))\n",
    "    return mu, scale\n",
    "\n",
    "def robust_scale_huber(x):\n",
    "    mu, scale = huber_estimator(x, delta=1.35)\n",
    "    return mu + 3 * scale\n",
    "\n",
    "\n",
    "y = robust_scale_huber(x) \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9469188303020584,\n",
       " 1.089635275166533,\n",
       " 1.1993804904035257,\n",
       " 1.2851357928430545,\n",
       " 1.3531919593050141,\n",
       " 1.4080824480231826,\n",
       " 1.453066019189784,\n",
       " 1.4904882792932141)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. generalized mean\n",
    "def generalized_mean(x, p=3):\n",
    "    x = np.asarray(x)\n",
    "    x = np.maximum(x, 0)  # if activations can be negative, use |x|\n",
    "    return (np.mean(x ** p)) ** (1.0 / p)\n",
    "\n",
    "y_3 = generalized_mean(x, 3)\n",
    "y_4 = generalized_mean(x, 4)\n",
    "y_5 = generalized_mean(x, 5)\n",
    "y_6 = generalized_mean(x, 6)\n",
    "y_7 = generalized_mean(x, 7)\n",
    "y_8 = generalized_mean(x, 8)\n",
    "y_9 = generalized_mean(x, 9)\n",
    "y_10 = generalized_mean(x, 10)\n",
    "\n",
    "y_3, y_4, y_5, y_6, y_7, y_8, y_9, y_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7434104681015015"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9. log sum exp \n",
    "def logsumexp_magnitude(x, beta=2):\n",
    "    x = np.abs(x)\n",
    "    m = np.max(x)\n",
    "    return (1.0 / beta) * (m + np.log(np.sum(np.exp(beta * (x - m)))))\n",
    "\n",
    "y = logsumexp_magnitude(x) \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8511439410330652\n",
      "8.36207962848076\n"
     ]
    }
   ],
   "source": [
    "# 10. high quantile gaussian\n",
    "import numpy as np\n",
    "from scipy.stats import norm, lognorm\n",
    "\n",
    "def high_quantile_gaussian(x, q=0.999):\n",
    "    x = np.abs(x)\n",
    "    mu, sigma = norm.fit(x)  # fit Gaussian\n",
    "    return norm.ppf(q, loc=mu, scale=sigma)\n",
    "\n",
    "def high_quantile_lognormal(x, q=0.999):\n",
    "    x = np.abs(x)\n",
    "    shape, loc, scale = lognorm.fit(x, floc=0)  # force loc=0 for magnitudes\n",
    "    return lognorm.ppf(q, shape, loc=loc, scale=scale)\n",
    "\n",
    "\n",
    "y = high_quantile_gaussian(x) \n",
    "print(y) \n",
    "y = high_quantile_lognormal(x) \n",
    "print(y) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2469263891038804\n"
     ]
    }
   ],
   "source": [
    "# 11. high quantile gpd\n",
    "from scipy.stats import genpareto\n",
    "\n",
    "def high_quantile_gpd(x, q=0.999, threshold_percentile=0.95):\n",
    "    x = np.abs(x)\n",
    "    threshold = np.percentile(x, threshold_percentile*100)  # peaks-over-threshold\n",
    "    exceedances = x[x > threshold] - threshold\n",
    "    c, loc, scale = genpareto.fit(exceedances, floc=0)\n",
    "    \n",
    "    # Predict the quantile\n",
    "    p_exceed = 1 - threshold_percentile\n",
    "    q_exceed = genpareto.ppf((q - threshold_percentile)/p_exceed, c, loc=loc, scale=scale)\n",
    "    return threshold + q_exceed\n",
    "\n",
    "y = high_quantile_gpd(x) \n",
    "print(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original max (first 5 channels): [8.58613278 7.09051212 8.8583193  7.658119   6.35609295]\n",
      "Shrunk max (first 5 channels): [7.97088074 7.62018385 8.10880594 7.74233665 7.49564283]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulated activations\n",
    "num_samples = 10\n",
    "num_channels = 20\n",
    "X = np.random.randn(num_samples, num_channels) * 2 + 5  # some random activations\n",
    "\n",
    "# Step 1: Compute per-channel max\n",
    "m_c = X.max(axis=0)  # shape: (num_channels,)\n",
    "\n",
    "# Step 2: Compute group-level statistics\n",
    "mu0 = np.mean(m_c)            # global mean\n",
    "tau2 = np.var(m_c)             # variance across channels\n",
    "\n",
    "# Step 3: Estimate per-channel variance (can use sample variance)\n",
    "sigma2_c = np.var(X, axis=0) / num_samples  # variance of the mean / max approximation\n",
    "\n",
    "# Step 4: Compute shrinkage weights\n",
    "w = sigma2_c / (sigma2_c + tau2)\n",
    "\n",
    "# Step 5: Shrink noisy maxima toward group mean\n",
    "m_c_shrunk = w * m_c + (1 - w) * mu0\n",
    "\n",
    "print(\"Original max (first 5 channels):\", m_c[:5])\n",
    "print(\"Shrunk max (first 5 channels):\", m_c_shrunk[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-channel scales (first 5): [6.70508961 8.45659568 8.93770539 9.21990798 8.97574586]\n"
     ]
    }
   ],
   "source": [
    "# 12. gmm cluster\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Simulated activations: 1000 samples, 512 channels\n",
    "num_samples = 10\n",
    "num_channels = 20\n",
    "X = np.random.randn(num_samples, num_channels) * 2 + 5  # example activations\n",
    "\n",
    "num_components = 3  # number of GMM components\n",
    "per_channel_scale = np.zeros(num_channels)\n",
    "\n",
    "for c in range(num_channels):\n",
    "    x_c = X[:, c].reshape(-1, 1)  # shape (num_samples, 1)\n",
    "    \n",
    "    # Fit GMM\n",
    "    gmm = GaussianMixture(n_components=num_components, random_state=0)\n",
    "    gmm.fit(x_c)\n",
    "    \n",
    "    # Identify tail cluster: cluster with largest mean\n",
    "    means = gmm.means_.flatten()\n",
    "    stds = np.sqrt(gmm.covariances_).flatten()\n",
    "    tail_idx = np.argmax(means)\n",
    "    \n",
    "    # Set scale: mean + 3*std of tail cluster\n",
    "    per_channel_scale[c] = means[tail_idx] + 3 * stds[tail_idx]\n",
    "\n",
    "print(\"Per-channel scales (first 5):\", per_channel_scale[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.70508961,  8.45659568,  8.93770539,  9.21990798,  8.97574586,\n",
       "       13.29053697,  7.17531501, 10.68463022,  8.23634335,  9.1881867 ,\n",
       "       11.2889807 ,  7.5935238 ,  7.73557123,  7.69539574,  7.99356908,\n",
       "        9.72632944, 11.47327141,  9.28671345,  8.47985396,  7.88969108])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_channel_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.12243265,  8.45359568,  8.21868415,  8.31262432,  8.97274586,\n",
       "       13.28753697,  6.42464874,  8.40435152,  6.84185469,  7.78277832,\n",
       "       10.05627357,  6.71632774,  7.36441428,  7.69239574,  7.19787954,\n",
       "        9.72332944, 10.58647726,  8.40949099,  8.47685396,  7.3711058 ])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-channel scales (first 5): [11.19930591 11.19650939 11.19784141 11.2010131  11.20617557]\n"
     ]
    }
   ],
   "source": [
    "# 15. empirical bayes + EVT\n",
    "import numpy as np\n",
    "from scipy.stats import genpareto\n",
    "\n",
    "def robust_per_channel_scale(X, tail_fraction=0.05, quantile=0.999):\n",
    "    N, C = X.shape\n",
    "    per_channel_scale = np.zeros(C)\n",
    "    \n",
    "    # Global mean for shrinkage\n",
    "    q_extremes = np.zeros(C)\n",
    "    tail_sizes = np.zeros(C)\n",
    "    \n",
    "    for c in range(C):\n",
    "        x_c = X[:, c]\n",
    "        threshold = np.percentile(x_c, 100*(1-tail_fraction))\n",
    "        tail = x_c[x_c > threshold] - threshold\n",
    "        tail_sizes[c] = len(tail)\n",
    "        \n",
    "        if len(tail) < 2:\n",
    "            q_extremes[c] = x_c.max()\n",
    "            continue\n",
    "        \n",
    "        xi, loc, beta = genpareto.fit(tail, floc=0)\n",
    "        k = len(tail)\n",
    "        # EVT extrapolated quantile\n",
    "        q_extreme = threshold + (beta / xi) * (((1 - quantile)/(k/N))**(-xi) - 1) if xi != 0 else threshold + beta * np.log((k/N)/(1-quantile))\n",
    "        q_extremes[c] = q_extreme\n",
    "    \n",
    "    # Empirical Bayes shrinkage across channels\n",
    "    mu0 = np.mean(q_extremes)\n",
    "    tau2 = np.var(q_extremes)\n",
    "    sigma2 = 1.0 / np.maximum(tail_sizes, 1)  # uncertainty ~ 1/n_tail\n",
    "    w = sigma2 / (sigma2 + tau2)\n",
    "    \n",
    "    per_channel_scale = w * q_extremes + (1-w) * mu0\n",
    "    return per_channel_scale\n",
    "\n",
    "# Example usage\n",
    "N = 512*128  # total tokens\n",
    "C = 512\n",
    "X = np.random.randn(N, C)*2 + 5\n",
    "scales = robust_per_channel_scale(X)\n",
    "print(\"Per-channel scales (first 5):\", scales[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.27780066, 12.74537227, 12.90488008, 13.09937988, 13.75556333])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.max(axis=0)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical Bayes scales (first 5): [13.58906863 13.58899879 13.58914105 13.58906048 13.5889502 ]\n"
     ]
    }
   ],
   "source": [
    "# 13. emipirical bayes\n",
    "import numpy as np\n",
    "\n",
    "def empirical_bayes_per_channel(X):\n",
    "    \"\"\"\n",
    "    Empirical Bayes per-channel scale.\n",
    "    X: numpy array of shape [num_tokens, num_channels]\n",
    "    Returns: per-channel scales (robust to noisy maxima)\n",
    "    \"\"\"\n",
    "    N, C = X.shape\n",
    "    \n",
    "    # Step 1: Per-channel max (or you can use RMS or mean + 3*std)\n",
    "    m_c = np.max(np.abs(X), axis=0)\n",
    "    \n",
    "    # Step 2: Estimate global statistics\n",
    "    mu0 = np.mean(m_c)        # global mean across channels\n",
    "    tau2 = np.var(m_c)         # between-channel variance\n",
    "    sigma2_c = np.var(X, axis=0) / N  # variance of the mean (approx)\n",
    "    \n",
    "    # Step 3: Compute shrinkage weights\n",
    "    w = sigma2_c / (sigma2_c + tau2)\n",
    "    \n",
    "    # Step 4: Shrink noisy per-channel maxima\n",
    "    per_channel_scale = w * m_c + (1 - w) * mu0\n",
    "    return per_channel_scale\n",
    "\n",
    "# Example usage\n",
    "N = 512 * 128  # total tokens\n",
    "C = 512\n",
    "X = np.random.randn(N, C)*2 + 5\n",
    "eb_scales = empirical_bayes_per_channel(X)\n",
    "print(\"Empirical Bayes scales (first 5):\", eb_scales[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.83080599, 13.44405546, 14.2285278 , 13.78095582, 13.17600823])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.max(axis=0)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVT scales (first 5): [11.17950945 11.19527866 11.15409209 11.32752952 11.20868582]\n"
     ]
    }
   ],
   "source": [
    "# 14. EVT\n",
    "from scipy.stats import genpareto\n",
    "\n",
    "def evt_per_channel_scale(X, tail_fraction=0.05, quantile=0.999):\n",
    "    \"\"\"\n",
    "    EVT per-channel scale estimation.\n",
    "    X: [num_tokens, num_channels]\n",
    "    tail_fraction: fraction of top activations to use as tail\n",
    "    quantile: target extreme quantile to extrapolate to\n",
    "    Returns: per-channel EVT scales\n",
    "    \"\"\"\n",
    "    N, C = X.shape\n",
    "    per_channel_scale = np.zeros(C)\n",
    "    \n",
    "    for c in range(C):\n",
    "        x_c = X[:, c]\n",
    "        \n",
    "        # Step 1: threshold for tail\n",
    "        threshold = np.percentile(x_c, 100*(1-tail_fraction))\n",
    "        tail = x_c[x_c > threshold] - threshold  # exceedances\n",
    "        \n",
    "        if len(tail) < 2:\n",
    "            # fallback if too few tail points\n",
    "            per_channel_scale[c] = x_c.max()\n",
    "            continue\n",
    "        \n",
    "        # Step 2: fit GPD\n",
    "        xi, loc, beta = genpareto.fit(tail, floc=0)\n",
    "        k = len(tail)\n",
    "        \n",
    "        # Step 3: extrapolate to desired quantile\n",
    "        if xi != 0:\n",
    "            q_extreme = threshold + (beta / xi) * (((1 - quantile)/(k/N))**(-xi) - 1)\n",
    "        else:\n",
    "            q_extreme = threshold + beta * np.log((k/N)/(1-quantile))\n",
    "        \n",
    "        per_channel_scale[c] = q_extreme\n",
    "        \n",
    "    return per_channel_scale\n",
    "\n",
    "# Example usage\n",
    "evt_scales = evt_per_channel_scale(X)\n",
    "print(\"EVT scales (first 5):\", evt_scales[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.83080599, 13.44405546, 14.2285278 , 13.78095582, 13.17600823])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.max(axis=0)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smoothquant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
